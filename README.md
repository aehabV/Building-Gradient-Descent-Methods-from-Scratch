[![Language Badge](https://img.shields.io/badge/Language-Python-3776ab.svg)](https://www.python.org/)
[![Library Badge](https://img.shields.io/badge/Library-NumPy-yellow.svg)](https://numpy.org/)
[![Algorithm Badge](https://img.shields.io/badge/Algorithms-Optimization-blue.svg)](https://en.wikipedia.org/wiki/Mathematical_optimization)
[![License Badge](https://img.shields.io/badge/License-CC%20BY--NC%204.0-0a2c46.svg)](https://creativecommons.org/licenses/by-nc/4.0/legalcode)

## Building Gradient Descent Methods from Scratch

This project implements various optimization algorithms using only NumPy in Python. The implemented algorithms include:

- Momentum
- AdaGrad
- RMSProp
- Adam

Additionally, the Broyden-Fletcher-Goldfarb-Shanno (BFGS) optimizer is also implemented. The project aims to conduct a comparative analysis of the results obtained from the BFGS optimizer and those obtained from using Adam.

The project involves the following steps:

- Implementing the optimization algorithms using NumPy.
- Implementing the BFGS optimizer.
- Conducting experiments to compare the results obtained from the BFGS optimizer and those obtained from using Adam.
- Analyzing the results and drawing conclusions.

Through this project, we hope to gain a deeper understanding of optimization algorithms and their performance in various scenarios.
